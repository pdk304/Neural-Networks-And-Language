# Neural Networks and Language
My work in computational linguistics at Robert Frank's lab CLAY and his course in neural networks and language.

## Project 1: Word Embeddings

![Word Graph](https://cdn.discordapp.com/attachments/584925805042335756/723680405860909167/5eb47c77c184ea00018a62d0.png)

Used word2vec and cosine similarity to examine how word embeddings encode meaning.

## Project 2: Language Model

Created a neural network language model from scratch using Numpy, with the the following architecture

![Network Architecture](https://cdn.discordapp.com/attachments/584925805042335756/723681905513332757/unknown.png)

## Project 3: Part of Speech Tagger
Implemented both a fully connected and bi-directional RNN network models for English part of speech tagging

<img src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png" width="800">

Image source: https://colah.github.io/posts/2015-08-Understanding-LSTMs/

## Project 4: Seq2seq grammar machine translation
Implemented an Attention-based encoder-decoder network for translating grammar phrases in English.


<img src="https://miro.medium.com/max/1400/1*A4H-IhqwjNZ_eL57Cqch0A.png" width="800">

Image source: https://towardsdatascience.com/day-1-2-attention-seq2seq-models-65df3f49e263
